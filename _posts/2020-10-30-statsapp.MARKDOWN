---
layout: post
title:  "Two Distinct Applications of Statistical Models"
date:   2020-10-30 10:30:57 -0400
categories: statistical models
---


## 1. Parameter Inference

 In the empirical or natural sciences, statistical models are generally used to evaluate point estimates of the parameters. The actual scale of a coefficient is important for scientific interpretation. From a broad array of potential models or inferential techniques, it is vital to use those that produce accurate parameter values in the absolute sense.
	
 Gathering data in this context can be time-intensive and costly. Often conducted under controlled conditions, the experimenter wants to learn as much as possible about the underlying process from a small sample size (N < 100). This led to a broad class of frequentist techniques that assume infinitely repeatable independent and identically disturbed (I.I.D.) processes. By extrapolating the data out into the infinite asymptote, a variety of tests can generalize parameter values from limited information.

 It matters not only that parameter estimates are accurate in the sense of converging to their actual value (“unbiased”), but also that the variability of this estimator has been minimized with respect to new data (“efficient”). Additional criteria include insensitivity to outlier errors ("robust"), uniqueness in value ("identfied"), and maximum information capture ("sufficient").
	
 Common tools of parameter inference:
	
 * t-tests, F-tests, and other hypothesis tests
 * p-values, significance levels, and confidence intervals for parameter estimates
 * Properties of the asymptotic distribution of parameters
 * OLS assumptions for linear models (best linear unbiased estimator) 
 * Parametric assumptions about the underlying data process (GLMs)

<br/><br/>

## 2. Data Prediction

 In economics, finance, marketing, or other real-world applications, often times only predictive accuracy on new data is important, not the properties of parameter values or their distribution. The exact parameters might be irrelevant to the point a model’s functional form , perhaps too complicated to express in a single equation.
	
 Prediction from data is measured by some loss function that assigns numeric penalty between predicted and actual target variables. For classification problems. Performance measurement can be fine . Often the modeler 
	
 To ensure the predictive ability . Cross-validation and other out-of-sample testing techniques 
	
 There are no requirements that the data represent a time-constant process that could be sampled repetable. and more recent market conditions is more salient to today's forecast.
	
 Common tools of predictive modeling:
 
 * More complicated and non-linear functional forms
 * Local interpolation (splines, nearest neighbor, clustering)
 * Iterative re-weighting or re-fitting model terms during estimation
 * Model aggregation and ensembles
 * "Black-box" algorithms too complex for a single equation (NNs and random forest)
 * Dimensionalty reduction due to large number of data features
 
	
